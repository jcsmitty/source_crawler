name: Crawl sources -> Feed

on:
  workflow_dispatch:
  schedule:
    - cron: "*/20 * * * *"  # every 20 minutes

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml gspread google-auth

      # Cache state so dedupe + ETag/Last-Modified persist between runs
      - name: Restore crawler state (sqlite)
        uses: actions/cache@v4
        with:
          path: crawler_state.sqlite
          key: crawler-state-${{ github.ref_name }}
          restore-keys: |
            crawler-state-

      - name: Run crawler
        env:
          SHEET_ID: ${{ secrets.SHEET_ID }}
          GOOGLE_SA_JSON_B64: ${{ secrets.GOOGLE_SA_JSON_B64 }}

          # Tabs
          WORKSHEET_MOVIES: "Movies"
          WORKSHEET_SOURCES: "Sources"
          WORKSHEET_FEED: "Feed"

          # Sensible target knobs
          MIN_DOMAIN_DELAY: "2.5"
          MAX_NEXT_PAGES_REVIEW: "10"
          MAX_NEXT_PAGES_AUTHOR: "6"
          MAX_NEXT_PAGES_HOME: "1"
          SITEMAP_TTL_HOURS: "24"
          SITEMAP_RECENT_DAYS: "14"
          MAX_NEW_PER_RUN: "2000"

          # Optional override
          USER_AGENT: "RTReviewFeedCrawler/2.1 (polite; contact: you@example.com)"
        run: |
          python crawl_to_sheet.py \
            --db crawler_state.sqlite \
            --max-new-per-run 2000

      - name: Save crawler state (sqlite)
        uses: actions/cache@v4
        with:
          path: crawler_state.sqlite
          key: crawler-state-${{ github.ref_name }}
