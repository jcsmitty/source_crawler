name: Crawl sources -> Feed (fast + resilient)

on:
  workflow_dispatch:
  schedule:
    - cron: "*/120 * * * *"  # every 20 minutes

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 19  # hard stop at job level

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml gspread google-auth urllib3

      - name: Restore crawler state (sqlite)
        uses: actions/cache@v4
        with:
          path: crawler_state.sqlite
          key: crawler-state-${{ github.ref_name }}
          restore-keys: |
            crawler-state-

      - name: Run crawler
        env:
          SHEET_ID: ${{ secrets.SHEET_ID }}
          GOOGLE_SA_JSON_B64: ${{ secrets.GOOGLE_SA_JSON_B64 }}

          WORKSHEET_MOVIES: "Movies"
          WORKSHEET_SOURCES: "Sources"
          WORKSHEET_FEED: "Feed"

          # Live status
          STATUS_EVERY_S: "15"

          # Runtime safety (stop before schedule overlap)
          MAX_RUN_SECONDS: "1080"  # 18 minutes

          # Politeness
          MIN_DOMAIN_DELAY: "2.0"
          JITTER_S: "0.8"

          # Pagination (moderate)
          MAX_NEXT_PAGES_REVIEW: "8"
          MAX_NEXT_PAGES_AUTHOR: "5"
          MAX_NEXT_PAGES_HOME: "1"

          # Candidate caps per page (movie-driven selection happens during extraction)
          MAX_CANDIDATES_PER_PAGE_HOME: "6"
          MAX_CANDIDATES_PER_PAGE_REVIEW: "12"
          MAX_CANDIDATES_PER_PAGE_AUTHOR: "10"

          # Candidate caps per source
          MAX_CANDIDATES_PER_SOURCE_HOME: "25"
          MAX_CANDIDATES_PER_SOURCE_REVIEW: "60"
          MAX_CANDIDATES_PER_SOURCE_AUTHOR: "45"

          # Movie filtering rules (workaround: nearby card text)
          REQUIRE_MOVIE_ON_HOME: "true"
          REQUIRE_MOVIE_ON_REVIEW: "false"
          REQUIRE_MOVIE_ON_AUTHOR: "false"

          # Article fetching limits
          ARTICLE_WORKERS: "4"
          MAX_ARTICLE_FETCHES_PER_RUN: "350"
          MAX_ARTICLES_PER_SOURCE: "12"

          # Cooldowns (skip domains that block Actions IPs or are unreachable)
          COOLDOWN_403_SECONDS: "3600"
          COOLDOWN_429_SECONDS: "1800"
          COOLDOWN_NETFAIL_SECONDS: "1800"

          USER_AGENT: "RTReviewFeedCrawler/2.3 (polite; contact: you@example.com)"
        run: |
          python crawl_to_sheet.py --db crawler_state.sqlite --max-new-per-run 2000

      - name: Save crawler state (sqlite)
        uses: actions/cache@v4
        with:
          path: crawler_state.sqlite
          key: crawler-state-${{ github.ref_name }}
