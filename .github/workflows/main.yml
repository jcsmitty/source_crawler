name: Crawl sources -> Feed (fast + live logs)

on:
  workflow_dispatch:
  schedule:
    - cron: "*/30 * * * *"

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml gspread google-auth

      - name: Restore crawler state (sqlite)
        uses: actions/cache@v4
        with:
          path: crawler_state.sqlite
          key: crawler-state-${{ github.ref_name }}
          restore-keys: |
            crawler-state-

      - name: Run crawler
        env:
          SHEET_ID: ${{ secrets.SHEET_ID }}
          GOOGLE_SA_JSON_B64: ${{ secrets.GOOGLE_SA_JSON_B64 }}

          WORKSHEET_MOVIES: "Movies"
          WORKSHEET_SOURCES: "Sources"
          WORKSHEET_FEED: "Feed"

          # speed/politeness knobs
          MIN_DOMAIN_DELAY: "2.0"
          MAX_NEXT_PAGES_REVIEW: "8"
          MAX_NEXT_PAGES_AUTHOR: "5"
          MAX_NEXT_PAGES_HOME: "1"

          # live status
          STATUS_EVERY_S: "15"

          # article concurrency + budget
          ARTICLE_WORKERS: "4"
          MAX_ARTICLE_FETCHES_PER_RUN: "500"
          REQUIRE_MOVIE_IN_ANCHOR_BEFORE_FETCH: "true"

          USER_AGENT: "RTReviewFeedCrawler/2.2 (polite; contact: you@example.com)"
        run: |
          python crawl_to_sheet.py --db crawler_state.sqlite --max-new-per-run 2000

      - name: Save crawler state (sqlite)
        uses: actions/cache@v4
        with:
          path: crawler_state.sqlite
          key: crawler-state-${{ github.ref_name }}
